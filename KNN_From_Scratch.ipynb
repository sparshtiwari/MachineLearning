{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN_From_Scratch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEKInw3MChMknOz/4NNItT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRZwJozUtVdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from math import sqrt\n",
        "from collections import Counter\n",
        "import random"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqBdQVWS1Igi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#in this project instead of using scikit learns KNN algo we are gonna define every single thing on our own from scratch (with a little help of numpy too :P)\n",
        "def knn(dataset,predict,k=3):\n",
        "  if len(dataset) >= k:\n",
        "    warnings.warn('Value of k must not be less than the total voting classes  ')\n",
        "  \n",
        "  distances = []\n",
        "  for group in dataset:\n",
        "    for features in dataset[group]:\n",
        "      euclid_dist = np.linalg.norm(np.array(features)-np.array(predict))# this is equivalent to euclidean_distance = sqrt( (features[0]-predict[0])**2 + (features[1]-predict[1])**2 )\n",
        "      distances.append([euclid_dist,group])\n",
        "  \n",
        "  votes = [i[1] for i in sorted(distances)[:k]]\n",
        "  vote_result = Counter(votes).most_common(1)[0][0]\n",
        "  return vote_result"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sVS6oJZCQeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Although in the above code segment we have tried our best and even used numpy to speed up the process and be as efficient as possible the main downfall of knn still can be seen from there that is we have to compare our prediction data to all the points from the dataset and as the dataset grows the algo will keep becoming slower and slower and there is almost no running away from that,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJMON5u44K3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lets try our knn on the breast cancer dataset used in the previous code\n",
        "df = pd.read_csv('/breast-cancer-wisconsin.data')\n",
        "df.replace('?',-99999, inplace=True)\n",
        "df.drop(['id'], 1, inplace=True)\n",
        "full_data = df.astype(float).values.tolist()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvtzeMtc5IIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Shuffling our data sp that there isn't any overfitting or biasing problem\n",
        "random.shuffle(full_data)\n",
        "test_size = 0.2\n",
        "train_set = {2:[], 4:[]}\n",
        "test_set = {2:[], 4:[]}\n",
        "train_data = full_data[:-int(test_size*len(full_data))]\n",
        "test_data = full_data[-int(test_size*len(full_data)):]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKO31oHs5U6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating our training and testing data also from scratch\n",
        "for i in train_data:\n",
        "    train_set[i[-1]].append(i[:-1])\n",
        "\n",
        "for i in test_data:\n",
        "    test_set[i[-1]].append(i[:-1])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbUUtV2_5qrn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0e76372-a1d6-46a6-9bf0-b36e6b829c89"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for group in test_set:\n",
        "    for data in test_set[group]:\n",
        "        vote = knn(train_set, data,k=5)\n",
        "        if group == vote:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "print('Accuracy:', correct/total)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9712230215827338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIshhRnq-4H3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#As you can see we have achieved similar accuracy as scikit learns own knn classifier but sill it won't perform that well on every use case and will be needed to tuned as per the demand."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}